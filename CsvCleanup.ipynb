{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a85b0b-8a1d-4882-b35a-7697a82e3b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[673, 33, 118, 314, 563]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark  \n",
    "sc = pyspark.SparkContext('local[*]') \n",
    "# do something to prove it works \n",
    "rdd = sc.parallelize(range(1000)) \n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316606b6-0668-4987-89e8-19b1098851b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV File Cleanup\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0efbb01-4946-44f7-9651-0f85d396afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"clean_me.csv\",sep=\"|\",header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f468b1-8c9b-4958-828d-88f87c4cf7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c490cc-5dda-4097-bbae-d5b551dded13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'order_id,delivery_company,quantity,price,ordered_date,address,,,,'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060f7271-ecb3-41c3-b4e3-dd321db8311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(column_name, \"col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9557ad06-e0c4-44b3-87a5-38a537bb618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 col|\n",
      "+--------------------+\n",
      "|1,delivery_comp_1...|\n",
      "|2,delivery_comp_2...|\n",
      "|3,delivery_comp_3...|\n",
      "|4,delivery_comp_0...|\n",
      "|5,delivery_comp_1...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6deb98-f381-4edf-926a-bdc6d29ac5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f636bc-2eda-47b4-8226-a7dc9bb16f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f13ace0-05fe-4b55-8920-779281c0d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_first2 = spark.sql(\"\"\"\n",
    " SELECT SPLIT(col, ',')[0] as order_id,\n",
    "        SPLIT(col, ',')[1] as delivery_company,\n",
    "        SPLIT(col, ',')[2] as quantity,\n",
    "        CONCAT_WS(',', SPLIT(col, ',')[3],\n",
    "        SPLIT(col, ',')[4], SPLIT(col, ',')[5],\n",
    "        SPLIT(col, ',')[6], SPLIT(col, ',')[7],\n",
    "        SPLIT(col, ',')[8], SPLIT(col, ',')[9]) as tail\n",
    " FROM temp \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9345afff-dcc9-48da-aeb2-c9cb54897bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_first2.createOrReplaceTempView(\"tmp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7c84e8-5b8d-43a4-bde6-4f7d630f5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the floating digits separated by , and add . to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5397987f-bf1d-4053-857d-4886ef992095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import split, regexp_replace, regexp_extract\n",
    "\n",
    "df_clean = clean_first2.withColumn(\"tail\",\n",
    "                        regexp_replace(\"tail\",r'(\\d+),(\\d+),',r'\\1.\\2,'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddbae4d6-8edb-45ec-97ed-01fc2343183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.createOrReplaceTempView(\"tmp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33478ae8-cc9a-4be7-957e-e6301aeb6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee22b117-520c-4070-99ff-7aa36db26556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = spark.sql(\"\"\"\n",
    "   SELECT order_id, delivery_company, quantity, \n",
    "   SPLIT(tail, ',')[0] as price,\n",
    "   SPLIT(tail, ',')[1] as ordered_date,\n",
    "        CONCAT_WS(',', SPLIT(tail, ',')[2],\n",
    "        SPLIT(tail, ',')[3], SPLIT(tail, ',')[4],\n",
    "        SPLIT(tail, ',')[5]) as tail\n",
    "   FROM (\n",
    "    SELECT order_id, delivery_company, quantity, regexp_replace(tail, \"-\", '/') as tail\n",
    "      from tmp3 ) tb\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20a1a8bd-aae5-499f-8987-3f1ae9aa3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the zipcode and state. I use tail as the part of the text left unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be42465b-6bd6-476f-8180-8847e48fe6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = (df_clean2\n",
    "             .withColumn(\"zipcode\", regexp_extract(\"tail\", r\"(\\d+)\", 0))\n",
    "             .withColumn(\"state\", regexp_extract(\"tail\", r\"\\b([A-Z]{2})\\b\", 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c21bb5-ba84-404f-8695-04c667ddd6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+------+------------+--------------------+-------+-----+\n",
      "|order_id|delivery_company|quantity| price|ordered_date|                tail|zipcode|state|\n",
      "+--------+----------------+--------+------+------------+--------------------+-------+-----+\n",
      "|       1| delivery_comp_1|       1|   1.2|    9/2/2022|Cedar Lane Housto...|  90001|   CA|\n",
      "|       2| delivery_comp_2|       2|   1.2|        null|Main Street,New Y...|  60601|   CA|\n",
      "|       3| delivery_comp_3|    null|   1.2|   14/3/2022|Main Street,Chica...|  10001|   TX|\n",
      "|       4| delivery_comp_0|       1|878.93|   20/4/2022|Oak Avenue,Los An...|  90001|   FL|\n",
      "|       5| delivery_comp_1|       2|   1.2|        null|Maple Drive Chica...|  60601|   FL|\n",
      "+--------+----------------+--------+------+------------+--------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03c71cf4-6729-41b7-a4db-f37956aad625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2.createOrReplaceTempView(\"tmp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406f6396-cd22-43b9-a246-51f9088d4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean3 = spark.sql(\"\"\"\n",
    "   SELECT order_id, delivery_company, CAST(quantity AS INTEGER) as quantity,price,state,\n",
    "   TO_DATE(ordered_date,\"d/m/yyyy\") as ordered_date,zipcode,\n",
    "   SPLIT(tail, ',')[0] as street,\n",
    "   SPLIT(tail, ',')[1] as city,tail\n",
    "   from tmp4 \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea348a92-752b-4949-88d5-f755457767ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean3.createOrReplaceTempView(\"tmp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7adc0493-c7f4-46b2-89ec-6c06ff58b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = spark.sql(\"\"\" SELECT city from (\n",
    "   SELECT city,count(1)\n",
    "   from tmp5 group by city order by 2 desc limit 5 ) tb\n",
    "\"\"\").rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70ef76ea-176d-426e-9b5d-5afbdabbf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the global cities variable to remove the cities from street and extract city name from the text\n",
    "# The top 5 is the only hardcoded variable that I had to infer based on the date in the CSV\n",
    "cities = [city.city for city in cities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4e0d49a-111d-4a21-9d83-1af023e71b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Miami', 'Chicago', 'Los Angeles', 'New York', 'Houston']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00dccfba-0b54-46fb-9c2b-93b827007248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def extract_city(sentence):\n",
    "    for city in cities:\n",
    "        if city in cities:\n",
    "            return city\n",
    "    return None\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def replace_city_empty(sentence):\n",
    "    for city in cities:\n",
    "        sentence = sentence.replace(city, \"\")\n",
    "    return sentence.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f939f834-bb98-436b-a0a6-545b043fe044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean3 = (df_clean3\n",
    "             .withColumn(\"city_\", extract_city(\"tail\"))\n",
    "             .withColumn(\"street_\", replace_city_empty(\"street\"))\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ed120de-8b59-4f9c-b869-d0025ab546c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+------+-----+------------+-------+-------------------+--------------------+--------------------+-----+-----------+\n",
      "|order_id|delivery_company|quantity| price|state|ordered_date|zipcode|             street|                city|                tail|city_|    street_|\n",
      "+--------+----------------+--------+------+-----+------------+-------+-------------------+--------------------+--------------------+-----+-----------+\n",
      "|       1| delivery_comp_1|       1|   1.2|   CA|  2022-01-09|  90001| Cedar Lane Houston|            CA 90001|Cedar Lane Housto...|Miami| Cedar Lane|\n",
      "|       2| delivery_comp_2|       2|   1.2|   CA|        null|  60601|        Main Street|   New York CA 60601|Main Street,New Y...|Miami|Main Street|\n",
      "|       3| delivery_comp_3|    null|   1.2|   TX|  2022-01-14|  10001|        Main Street|    Chicago TX 10001|Main Street,Chica...|Miami|Main Street|\n",
      "|       4| delivery_comp_0|       1|878.93|   FL|  2022-01-20|  90001|         Oak Avenue|Los Angeles FL 90001|Oak Avenue,Los An...|Miami| Oak Avenue|\n",
      "|       5| delivery_comp_1|       2|   1.2|   FL|        null|  60601|Maple Drive Chicago|            FL 60601|Maple Drive Chica...|Miami|Maple Drive|\n",
      "+--------+----------------+--------+------+-----+------------+-------+-------------------+--------------------+--------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3ac1de6-b997-4d3d-a88f-1026ddaca5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean3.createOrReplaceTempView(\"tmp6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b837a317-181b-4b8f-b2fc-95a0a65543af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e25e0e3d-3f31-4e8e-b7bc-ee0fa7591582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+------+------------+-------+-----------+-----+-----+\n",
      "|order_id|delivery_company|quantity| price|ordered_date|zipcode|     street| city|state|\n",
      "+--------+----------------+--------+------+------------+-------+-----------+-----+-----+\n",
      "|     105| delivery_comp_1|     1.5|   1.2|  2022-01-27|  90001|Maple Drive|Miami|   NY|\n",
      "|    2433| delivery_comp_1|     1.5|980.53|  2022-01-27|  60601| Cedar Lane|Miami|   TX|\n",
      "|     305| delivery_comp_1|     2.0|   1.2|  2022-01-27|  77001| Oak Avenue|Miami|   NY|\n",
      "|    1157| delivery_comp_1|     2.0|   1.2|  2022-01-27|  10001| Oak Avenue|Miami|   FL|\n",
      "|    1173| delivery_comp_1|     1.5|   1.2|  2022-01-27|  10001| Cedar Lane|Miami|   TX|\n",
      "|    1209| delivery_comp_1|     1.5|   1.2|  2022-01-27|  33101|Maple Drive|Miami|   TX|\n",
      "|    1353| delivery_comp_1|     1.5| 21.58|  2022-01-27|  33101| Oak Avenue|Miami|   TX|\n",
      "|    1361| delivery_comp_1|     2.0|   1.2|  2022-01-27|  90001|Maple Drive|Miami|   TX|\n",
      "|    1821| delivery_comp_1|     1.5|139.17|  2022-01-27|  77001| Cedar Lane|Miami|   TX|\n",
      "|    2309| delivery_comp_1|     2.0|173.17|  2022-01-27|  90001|Main Street|Miami|   TX|\n",
      "+--------+----------------+--------+------+------------+-------+-----------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "        SELECT \n",
    "          order_id, delivery_company,\n",
    "        CASE WHEN quantity IS NULL THEN avg_quantity \n",
    "             ELSE quantity END as quantity,\n",
    "             price,\n",
    "             CASE WHEN ordered_date IS NULL THEN next_date\n",
    "                  WHEN ordered_date IS NULL AND next_date IS NULL THEN LAST_DAY(prev_date)\n",
    "                  ELSE ordered_date \n",
    "                  END as ordered_date,\n",
    "             zipcode,street,city,state \n",
    "          FROM\n",
    "      (SELECT \n",
    "      order_id, delivery_company, quantity,price,ordered_date,zipcode,city_ as city,street_ as street, state,\n",
    "      avg(quantity) OVER (PARTITION BY delivery_company) as avg_quantity,\n",
    "      LEAD(ordered_date) OVER (PARTITION BY delivery_company ORDER BY ordered_date asc) as next_date,\n",
    "      LAG(ordered_date) OVER (PARTITION BY delivery_company ORDER BY ordered_date asc) as prev_date\n",
    "      FROM tmp6) tb  where delivery_company = 'delivery_comp_1' order by ordered_date desc\n",
    "      \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e436eb0-be99-4f63-a43b-d7b52e1fa41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
